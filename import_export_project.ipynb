{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics                           \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import datasets, neighbors\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict, StratifiedKFold, learning_curve)\n",
    "\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# one hot encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "import statistics\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Databases used from FAOSTAT about agriculture https://www.fao.org/faostat/en/#data\n",
    "\n",
    "# Import and export data \n",
    "full_data = pd.read_csv(\"Trade_CropsLivestock.csv\",encoding='latin-1')\n",
    "\n",
    "# Temperature data \n",
    "temp = pd.read_csv(\"temperature_dataset.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for import and export dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Denmark\", \"Finland\", \"Norway\", \"Ireland\"]\n",
    "\n",
    "\n",
    "filtered_df = full_data[full_data['Area'].isin(countries)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with information about import and export data\n",
    "\n",
    "full_data = filtered_df[[\"Area\", \"Element\", \"Item\", \"Year\",\"Unit\", \"Value\"]]\n",
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for temperature dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df = temp[temp['Area'].isin(countries)]\n",
    "temperature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temperature_df[[\"Area\", \"Year\", \"Element\", \"Unit\", \"Value\"]]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = temp.query(\"Element=='Temperature change'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = temp.query(\"Element=='Standard Deviation'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_merged = pd.merge(temp1, temp2, on=[\"Area\",\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_merged=temp_merged.rename(columns={\"Unit_x\":\"Unit\",\"Value_x\":\"Temperature\",\"Value_y\":\"Std_deviation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = temp_merged[[\"Area\", \"Year\", \"Temperature\"]]\n",
    "temp_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging import export and temperature datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(full_data, temp_final, on=[\"Area\",\"Year\"])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ton = final_df.query(\"Element=='Import Quantity'\")\n",
    "imp_ton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_value= final_df.query(\"Element=='Import Value'\")\n",
    "imp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_df = pd.merge(imp_ton, imp_value, on=[\"Area\",\"Year\",\"Item\",\"Temperature\"])\n",
    "import_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_list = [\"import_quantity\", \"import_qtd_value\", \"import_unit_value\", \"import_value\"]\n",
    "export_list = [\"export_quantity\", \"export_qtd_value\", \"export_unit_value\", \"export_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_final=import_df.rename(columns={\"Unit_x\":\"import_quantity\",\n",
    "                                       \"Value_x\":\"import_qtd_value\",\n",
    "                                       \"Unit_y\":\"import_unit_value\",\n",
    "                                       \"Value_y\":\"import_value\",\n",
    "                                      })\n",
    "import_dataset = import_final[[\"Area\", \"Year\", \"Item\", \"import_quantity\", \"import_qtd_value\", \n",
    "                \"import_unit_value\", \"import_value\", \"Temperature\" ]]\n",
    "\n",
    "import_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ton = final_df.query(\"Element=='Export Quantity'\")\n",
    "exp_value = final_df.query(\"Element=='Export Value'\")\n",
    "export_df = pd.merge(exp_ton, exp_value, on=[\"Area\",\"Year\",\"Item\",\"Temperature\"])\n",
    "export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_final=export_df.rename(columns={\"Unit_x\":\"export_quantity\",\n",
    "                                       \"Value_x\":\"export_qtd_value\",\n",
    "                                       \"Unit_y\":\"export_unit_value\",\n",
    "                                       \"Value_y\":\"export_value\",\n",
    "                                      })\n",
    "export_dataset = export_final[[\"Area\", \"Year\", \"Item\", \"export_quantity\", \"export_qtd_value\", \n",
    "                \"export_unit_value\", \"export_value\", \"Temperature\" ]]\n",
    "\n",
    "export_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df = pd.merge(import_dataset, export_dataset, on=[\"Area\",\"Year\",\"Item\",\"Temperature\"])\n",
    "imp_exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df = imp_exp_df[[\"Area\", \"Year\", \"Item\", \"import_quantity\", \"import_qtd_value\", \n",
    "                \"import_unit_value\", \"import_value\", \"export_quantity\", \"export_qtd_value\", \n",
    "                \"export_unit_value\", \"export_value\", \"Temperature\" ]]\n",
    "imp_exp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of missing values\n",
    "round(100*(imp_exp_df.isnull().sum()/len(imp_exp_df.index)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data missing amount is too small, will remove all \n",
    "\n",
    "# Removing NaN rows\n",
    "imp_exp_df = imp_exp_df[~np.isnan(imp_exp_df['import_qtd_value'])]\n",
    "imp_exp_df = imp_exp_df[~np.isnan(imp_exp_df['export_qtd_value'])]\n",
    "imp_exp_df = imp_exp_df[~np.isnan(imp_exp_df['export_value'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(imp_exp_df.isnull().sum()/len(imp_exp_df.index)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for categorical data\n",
    "imp_exp_df.nunique(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising continuous features for our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = imp_exp_df[['import_qtd_value','import_value','export_qtd_value', 'export_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df = imp_exp_df.drop(['import_qtd_value','import_value','export_qtd_value', 'export_value'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df = pd.concat([imp_exp_df,normalized_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting our dataset outside of jupyter notebook\n",
    "\n",
    "imp_exp_df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining single datasets for each country for further comparison \n",
    "\n",
    "ireland_df = imp_exp_df.query(\"Area=='Ireland'\")\n",
    "denmark_df = imp_exp_df.query(\"Area=='Denmark'\")\n",
    "norway_df = imp_exp_df.query(\"Area=='Norway'\")\n",
    "finland_df = imp_exp_df.query(\"Area=='Finland'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = imp_exp_df.copy()\n",
    "data5['import_value'] = (data5['import_value']-data5['import_value'].mean())/data5['import_value'].std()\n",
    "data5['import_value'].head()\n",
    "sns.distplot(data5['import_value']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = imp_exp_df.copy()\n",
    "data5['export_value'] = (data5['export_value']-data5['export_value'].mean())/data5['export_value'].std()\n",
    "data5['export_value'].head()\n",
    "sns.distplot(data5['export_value']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = imp_exp_df[[\"import_value\",\"export_value\", \"Temperature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between features \n",
    "correlation = ireland_df.corr()\n",
    "\n",
    "# plotting heatmap to see the correlation for better understanding\n",
    "plt.figure(figsize = (9,7))\n",
    "sns.heatmap(correlation, annot = True, linecolor = \"white\",lw=0.5, cmap = \"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scatter plot of balance and values variable in data\n",
    "plt.scatter(imp_exp_df.import_value,imp_exp_df.Year)\n",
    "plt.show()\n",
    "\n",
    "#plot the scatter plot of balance and values variable in data\n",
    "imp_exp_df.plot.scatter(x=\"import_value\",y=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scatter plot of balance and values variable in data\n",
    "plt.scatter(imp_exp_df.export_value,imp_exp_df.Year)\n",
    "plt.show()\n",
    "\n",
    "#plot the scatter plot of balance and values variable in data\n",
    "imp_exp_df.plot.scatter(x=\"export_value\",y=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imp_exp_df\n",
    "X = dataset.iloc[:, [9, 11]].values\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(max_depth = 10, random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate cm by calling a method named as 'confusion_matrix'\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Call a method heatmap() to plot confusion matrix\n",
    "sns.heatmap(cm, annot = True)\n",
    "\n",
    "# print the classification_report based on y_test and y_predict\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying KNN Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With no pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, [9, 11]].values\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(7)\n",
    "knn.fit(X_train,y_train)\n",
    "print(\"Train score before PCA\",knn.score(X_train,y_train),\"%\")\n",
    "print(\"Test score before PCA\",knn.score(X_test,y_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA function on training and testing set of X component\n",
    "\n",
    "# Create and initialise an object (pca) by calling a method PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Transform the data into traning and testing\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    " \n",
    "# Store the explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,len(pca.explained_variance_ ) + 1), pca.explained_variance_ )\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1, len(pca.explained_variance_ ) + 1), pca.explained_variance_,\n",
    "         c = 'red',\n",
    "         label = \"Cumulative Explained Variance\")\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)\n",
    "X_new=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca = KNeighborsClassifier(7)\n",
    "knn_pca.fit(X_train_new,y_train)\n",
    "print(\"Train score after PCA\",knn_pca.score(X_train_new,y_train),\"%\")\n",
    "print(\"Test score after PCA\",knn_pca.score(X_test_new,y_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = imp_exp_df[['Area','import_qtd_value','import_value','export_qtd_value', 'export_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into X and y\n",
    "X = data_df.drop(\"Area\", axis = 1)\n",
    "y = data_df.import_value.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "# scaling the features\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Biulding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using rbf kernel, C=1, default value of gamma\n",
    "\n",
    "model = SVC(C = 1, kernel='sigmoid')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# precision\n",
    "print(\"precision\", metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "# recall/sensitivity\n",
    "print(\"recall\", metrics.recall_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV method to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# specify range of hyperparameters\n",
    "# Set the parameters by cross-validation\n",
    "hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "# specify model\n",
    "model = SVC(kernel=\"rbf\")\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = model, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'accuracy', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, y_train)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting C to numeric type for plotting on x-axis\n",
    "cv_results['param_C'] = cv_results['param_C'].astype('int')\n",
    "\n",
    "# # plotting\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# subplot 1/3\n",
    "plt.subplot(131)\n",
    "gamma_01 = cv_results[cv_results['param_gamma']==0.01]\n",
    "\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.01\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "# subplot 2/3\n",
    "plt.subplot(132)\n",
    "gamma_001 = cv_results[cv_results['param_gamma']==0.001]\n",
    "\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.001\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "# subplot 3/3\n",
    "plt.subplot(133)\n",
    "gamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n",
    "\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.0001\")\n",
    "plt.ylim([0.80, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "best_score = model_cv.best_score_\n",
    "best_hyperparams = model_cv.best_params_\n",
    "\n",
    "print(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimal hyperparameters\n",
    "best_params = {\"C\": 100, \"gamma\": 0.0001, \"kernel\":\"rbf\"}\n",
    "\n",
    "# model\n",
    "model = SVC(C=100, gamma=0.0001, kernel=\"rbf\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"precision\", metrics.precision_score(y_test, y_pred))\n",
    "# print(\"sensitivity/recall\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paiwise scatter plot\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.pairplot(cv_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "cor = cv_results.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "model_cv = GridSearchCV(estimator = ridge, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "model_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results = cv_results[cv_results['param_alpha']<=200]\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scores with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 15\n",
    "ridge = Ridge(alpha=alpha)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['import_value']\n",
    "X = df.drop(['import_value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LinearRegression()\n",
    "lrm.fit(X_train, Y_train) #fit an OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_train = lrm.predict(X_train)\n",
    "y_preds_test = lrm.predict(X_test)  #making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared of the model in training set is: {}\".format(lrm.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(lrm.score(X_test, Y_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(Y_test, y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test - y_preds_test) / Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GridSearch for parameter optimization\n",
    "lassoregr = GridSearchCV(Lasso(),\n",
    "                    param_grid={\n",
    "                        'alpha': [0.01, 0.1, 1]\n",
    "                    }, verbose=1)\n",
    "\n",
    "lassoregr.fit(X_train, Y_train)\n",
    "\n",
    "lasso = lassoregr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are making predictions here\n",
    "y_preds_train = lasso.predict(X_train)\n",
    "y_preds_test_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model in training set is: {}\".format(lasso.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(lasso.score(X_test, Y_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(Y_test, y_preds_test_lasso)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test - y_preds_test_lasso) / Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_random = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"R-squared of the model in training set is: {}\".format(regressor.score(X_train, Y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(regressor.score(X_test, Y_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(mse(Y_test, y_pred_random)**(1/2)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((Y_test - y_pred_random) / Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to compare de results\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "table = [['Statistics', 'SVM', 'Linear Regression', 'Lasso Regression', 'Random Florest','Decision Tree', 'KNN Neighbors'], \n",
    "         ['Accuracy', 0.93, 0.55, 0.55, 0.90, 0.42, 0.53], \n",
    "         ['R-squared',0, 0.5, 0.50, 0.90, 0, 0],\n",
    "         ['Root mean', 0, 0.67, 0.67, 0.30, 0, 0],\n",
    "         ['Mean abs error', 0, 0.51, 0.51, 0.72, 0, 0]]\n",
    "tab = PrettyTable(table[0])\n",
    "tab.add_rows(table[1:])\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
